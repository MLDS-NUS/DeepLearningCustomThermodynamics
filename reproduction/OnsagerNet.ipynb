{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-06 19:03:01.050399: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-09-06 19:03:01.060610: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-09-06 19:03:01.060624: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/eleonore/.local/lib/python3.8/site-packages/tensorflow/python/compat/v2_compat.py:107: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    }
   ],
   "source": [
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_FORCE_GPU_ALLOW_GROWTH']='true'\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_random_seed(0)\n",
    "np.random.seed(seed=0 )\n",
    "train_data=10000\n",
    "test_data=1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_tra=610\n",
    "T_step=1001\n",
    "N_bead=300\n",
    "dim_x=900\n",
    "N_conf=0\n",
    "dic='../data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_meta=pickle.load(open(dic+'ex_train.pkl', 'rb'))\n",
    "Z1_ex=data_meta['Wi5'][:N_tra,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Readme': 'Contains the molecule extension with time for trajectories under a certain field strength (Wi = 5). There are 800 rows and 1001 columns in the array. Each row corresponds to a single trajectory and each column corresponds to a time point (time = 0 in the first column and time = end in the last column). Each trajectory corresponds to a different initial condition. The initial conditions were chosen based on anticipated predisposition to going through metastable state during stretching process.',\n",
       " 'Wi5': array([[ 24.32,  22.82,  25.97, ..., 240.44, 242.66, 244.56],\n",
       "        [ 20.25,  19.39,  16.2 , ..., 248.11, 248.43, 248.84],\n",
       "        [ 16.79,  16.98,  19.58, ..., 246.08, 246.45, 245.27],\n",
       "        ...,\n",
       "        [ 76.33,  74.22,  74.56, ..., 240.74, 238.43, 238.99],\n",
       "        [ 76.33,  75.51,  77.19, ..., 243.79, 244.63, 247.04],\n",
       "        [ 76.33,  75.46,  74.51, ..., 243.72, 245.37, 247.97]])}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "208.21994500581388 69.79624239435213\n"
     ]
    }
   ],
   "source": [
    "Z_mean=Z1_ex.mean()\n",
    "Z_std=Z1_ex.std()\n",
    "print(Z_mean,Z_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "enter name of result folder:\n",
      "default folder is ./checkpoint/\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    res_dir = input('enter name of result folder:')\n",
    "    if res_dir == '':\n",
    "        res_dir = 'checkpoints' #default folder\n",
    "        print(\"default folder is ./checkpoint/\")\n",
    "    try:\n",
    "        os.mkdir(res_dir)\n",
    "        break;\n",
    "    except OSError:\n",
    "        print('Results will be stored in existing or default folder - previous results may be overwritten\\n')\n",
    "        break;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize the weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xavier_init(size):\n",
    "    in_dim = size[0]\n",
    "    out_dim = size[1]        \n",
    "    xavier_stddev = np.sqrt(2.0/(in_dim + out_dim))\n",
    "    return tf.Variable(tf.truncated_normal([in_dim, out_dim], stddev=xavier_stddev,dtype=tf.float32,seed=0), dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xavier_init_small(size):\n",
    "    in_dim = size[0]\n",
    "    out_dim = size[1]        \n",
    "    xavier_stddev = 0.01*np.sqrt(2.0/(in_dim + out_dim))\n",
    "    return tf.Variable(tf.truncated_normal([in_dim, out_dim], stddev=xavier_stddev,dtype=tf.float32,seed=0), dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RePU(inputs):\n",
    "    \"\"\"Rectified Power Unit Activation\n",
    "\n",
    "    z->max(0,z)^p\n",
    "    \"\"\"\n",
    "    return (tf.nn.relu(inputs)  )**2\n",
    "\n",
    "\n",
    "def ShiftedRePU(inputs):\n",
    "    \"\"\"Shifted Rectified Power Unit Activation\n",
    "\n",
    "    z->max(0,z)^p - max(0,z-0.5)^p\n",
    "    \"\"\"\n",
    "    g=(tf.nn.relu(inputs)  )**2-(tf.nn.relu(inputs-0.5)  )**2\n",
    "    return g"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "generate $\\phi$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural network for potential term\n",
    "$V(x)=\\beta  ||x||^2 +\\frac{1}{2} \\sum_{i=1}^{m} (U_i(x)+\\sum_{j=1}^m \\gamma_{i,j}x_i)^2$\n",
    "where $U_i(x)=w \\phi_i(x)+b$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neural_potential(X, weights, biases,Gamma,beta):\n",
    "    num_layers = len(weights) + 1  \n",
    "    H=X\n",
    "    for l in range(0,num_layers-2):\n",
    "        W = weights[l]\n",
    "        b = biases[l]\n",
    "        H = ShiftedRePU(tf.add(tf.matmul(H, W), b))\n",
    "    W = weights[-1]\n",
    "    b = biases[-1]\n",
    "    Y = 1/2*tf.reduce_sum( ( tf.matmul(X, Gamma[0])   +(tf.add(tf.matmul(H, W), b)) )**2 ,1, keepdims=True)+beta[0]*tf.reduce_sum(X**2, axis=1, keepdims=True)\n",
    "    return Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural network for M and W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neural_A(X, weights, biases ):\n",
    "    num_layers = len(weights) + 1  \n",
    "    H=X\n",
    "    for l in range(0,num_layers-1):\n",
    "        W = weights[l]\n",
    "        b = biases[l]\n",
    "        H = tf.nn.tanh(tf.add(tf.matmul(H, W), b))\n",
    "    return H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SymmAntiDecomposition( inputs,dim_low):\n",
    "    A = tf.reshape(inputs, [-1, dim_low, dim_low])\n",
    "    lower_triangle = tf.linalg.band_part(A, -1, 0)\n",
    "    upper_triangle = tf.linalg.band_part(A, 0, -1)\n",
    "    symmetric = lower_triangle @ tf.transpose(lower_triangle, [0, 2, 1])\n",
    "    antisymmetric = upper_triangle - tf.transpose(upper_triangle,[0, 2, 1])\n",
    "    return symmetric, antisymmetric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neural_SymmAnti(X,  weights_A, biases_A,dim_low):\n",
    "    A=neural_A(X, weights_A, biases_A)\n",
    "    M,W=SymmAntiDecomposition( A,dim_low)\n",
    "    return M,W "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural network for force term :$f(x)=wx+b$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neural_force(X, weights, biases ):\n",
    "    num_layers = len(weights) + 1  \n",
    "    H=X\n",
    "    for l in range(0,num_layers-2):\n",
    "        W = weights[l]\n",
    "        b = biases[l]\n",
    "        H = tf.nn.tanh(tf.add(tf.matmul(H, W), b))\n",
    "    W = weights[-1]\n",
    "    b = biases[-1]\n",
    "    Y =  tf.add(tf.matmul(H, W), b)\n",
    "    return Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural network for the drift term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neural_RHS(X, weights_potential, biases_potential,Gamma,beta , weights_force, biases_force,weights_A, biases_A,dim_low, alpha):\n",
    "    \"\"\"Combination Layer for OnsagerNet\n",
    "\n",
    "    Takes as input a tuple [M, W, g, f] and outputs\n",
    "    - (M + W) g - alpha * g + f\n",
    "    \"\"\" \n",
    "    M,W=neural_SymmAnti(X,  weights_A, biases_A,dim_low)\n",
    "    V=neural_potential(X, weights_potential, biases_potential,Gamma,beta)\n",
    "    V_x= tf.gradients(V, X)[0]\n",
    "    f=neural_force(X, weights_force, biases_force )  \n",
    "    rhs=-tf.einsum('ijk,ik->ij', M + W, V_x) - alpha * V_x +0* f\n",
    "    return rhs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural network for the low dimension:PCA or autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neural_sample(X, weights, biases ):\n",
    "    num_layers = len(weights) + 1  \n",
    "    H=X\n",
    "    for l in range(0,num_layers-2):\n",
    "        W = weights[l]\n",
    "        b = biases[l]\n",
    "        H = tf.nn.tanh(tf.add(tf.matmul(H, W), b))\n",
    "    W = weights[-1]\n",
    "    Y =   tf.matmul(H, W) \n",
    "    return Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data(path,N0,N1):\n",
    "    m=0\n",
    "    N_tra=N1-N0\n",
    "    a_tra=np.linspace(N0,N1-1,N1-N0,dtype=int)\n",
    "    data = pickle.load(open(path, 'rb'))\n",
    "    a_test=np.linspace(0,2,3 ,dtype=int)\n",
    "    X=np.zeros([T_step ,3*N_bead*N_tra])\n",
    "    for i in a_tra:\n",
    "        W_1=data['Trajectory' + str(i+1+500*m)  ]\n",
    "        for j in range(N_bead):\n",
    "            XX=np.reshape(W_1[:,j],[1001,3])\n",
    "            X[:,a_test+3*j+N_bead*3*i]=XX\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data_1trajectory(path,N0,N1):\n",
    "    N_tra=N1-N0\n",
    "    a_tra=np.linspace(N0,N1-1,N1-N0,dtype=int)\n",
    "    data = pickle.load(open(path, 'rb'))\n",
    "    a_test=np.linspace(0,2,3 ,dtype=int)\n",
    "    X_all_t0=np.zeros([(T_step-1)*N_tra ,3*N_bead])\n",
    "    X_all_t1=np.zeros([(T_step-1)*N_tra ,3*N_bead])\n",
    "    X_all=np.zeros([T_step*N_tra ,3*N_bead])\n",
    "    L_test=np.linspace(0,T_step-1,T_step,dtype=int)\n",
    "    for i in a_tra:\n",
    "        X_all_1=np.zeros([T_step ,3*N_bead])\n",
    "        W_1=data['Trajectory' + str(i+1) ]\n",
    "        for j in range(N_bead):\n",
    "            XX=np.reshape(W_1[:,j],[1001,3])\n",
    "            X_all_1[:,a_test+3*j]=XX\n",
    "        X_all[(i-N0)*T_step+L_test,:]=X_all_1\n",
    "        X_all_t0[(i-N0)*(T_step-1)+L_test[:-1],:]=X_all_1[:-1,:]\n",
    "        X_all_t1[(i-N0)*(T_step-1)+L_test[:-1],:]=X_all_1[1:,:]\n",
    "    del(data)\n",
    "    del(W_1)\n",
    "    return X_all_t0,X_all_t1,X_all\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_Z1_1trajectory(path,N0,N1):\n",
    "    N_tra=N1-N0\n",
    "    conf=0\n",
    "    data = pickle.load(open(path, 'rb'))\n",
    "    Z1_trajectory_train=data['Wi5' ]\n",
    "    Z1_trajectory_train=(Z1_trajectory_train-Z_mean)/Z_std\n",
    "    T_test=np.linspace(N0,N1-1,N1-N0,dtype=int)\n",
    "    Z1_all=np.reshape(Z1_trajectory_train[conf*N_tra+T_test,:],[N_tra*T_step,1])\n",
    "    Z1_all_t0=np.reshape(Z1_trajectory_train[conf*N_tra+T_test,:-1],[N_tra*(T_step-1),1])\n",
    "    Z1_all_t1=np.reshape(Z1_trajectory_train[conf*N_tra+T_test,1:],[N_tra*(T_step-1),1])\n",
    "    del(data)\n",
    "    del(Z1_trajectory_train)\n",
    "    return Z1_all_t0,Z1_all_t1,Z1_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neural_autoencoder(X, weights, biases ):\n",
    "    num_layers = len(weights) + 1  \n",
    "    H=X\n",
    "    for l in range(0,num_layers-2):\n",
    "        W = weights[l]\n",
    "        b = biases[l]\n",
    "        H = tf.nn.tanh(tf.add(tf.matmul(H, W), b))\n",
    "    W = weights[-1]\n",
    "    b = biases[-1]\n",
    "    H =  (tf.add(tf.matmul(H, W), b))\n",
    "    return H"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NN for potential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim_low=3\n",
    "dim_output=50\n",
    "\n",
    "layers = [dim_low ] +1* [128] + [dim_output ]\n",
    "L = len(layers)\n",
    "weights_potential = [xavier_init([layers[l], layers[l+1]]) for l in range(0, L-1)]    \n",
    "biases_potential = [tf.Variable( tf.zeros((1, layers[l+1]),dtype=tf.float32)) for l in range(0, L-1)]\n",
    "\n",
    "layers_L = [dim_low ] +   [dim_output ]\n",
    "Gamma = [xavier_init([layers_L[0], layers_L[-1]]) ]\n",
    "beta=[tf.exp(  tf.Variable(0,dtype=tf.float32,trainable=True) )]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NN for A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers_A= [dim_low]   +2*[20]+ [dim_low *dim_low]\n",
    "L_A = len(layers_A)\n",
    "weights_A= [xavier_init([layers_A[l], layers_A[l+1]])  for l in range(0, L_A-1)]    \n",
    "biases_A = [tf.Variable( tf.zeros((1, layers_A[l+1]),dtype=tf.float32)) for l in range(0, L_A-1)]   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NN for force"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers_force = [dim_low] + 0* [20] + [dim_low]\n",
    "L_force = len(layers_force)\n",
    "weights_force = [xavier_init([layers_force[l], layers_force[l+1]]) for l in range(0, L_force-1)]   \n",
    "biases_force = [tf.Variable( tf.zeros((1, layers_force [l+1]),dtype=tf.float32)) for l in range(0, L_force -1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NN for low dimension # PCA or auto encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "W=np.loadtxt(dic+'eig_whiten.txt')\n",
    "eig_vecs=np.loadtxt(dic+'eig_vecs.txt')\n",
    "eig_vals=np.loadtxt(dic+'eig_vals.txt')\n",
    "pca_1=W[:,:dim_low-1]\n",
    "weights_L=[tf.Variable(np.array(pca_1),dtype=tf.float32,trainable=False)]\n",
    "layers_low = [3*N_bead] + 0* [20] + [dim_low-1]\n",
    "L_low = len(layers_low)\n",
    "weights_L = [xavier_init([layers_low[l], layers_low[l+1]]) for l in range(0, L_low-1)]   \n",
    "biases_L= [tf.Variable( tf.zeros((1, layers_low[l+1]),dtype=tf.float32 )) for l in range(0, L_low -1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers_encoder = [3*N_bead] + [16]+  [dim_low-1]\n",
    "L_encoder = len(layers_encoder)\n",
    "weights_encoder = [xavier_init_small([layers_encoder[l], layers_encoder[l+1]]) for l in range(0, L_encoder-1)]   \n",
    "biases_encoder= [tf.Variable( tf.zeros((1, layers_encoder[l+1]),dtype=tf.float32 )) for l in range(0, L_encoder -1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers_decoder = [dim_low-1]+[16]+[3*N_bead]\n",
    "L_decoder = len(layers_decoder)\n",
    "weights_decoder = [xavier_init_small([layers_decoder[l], layers_decoder[l+1]]) for l in range(0, L_decoder-1)]   \n",
    "biases_decoder= [tf.Variable( tf.zeros((1, layers_decoder[l+1]),dtype=tf.float32 )) for l in range(0, L_decoder -1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "t0=0\n",
    "t1=0.5\n",
    "dt=(t1-t0)/(T_step)\n",
    "vect=np.linspace(t0,t1,T_step+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigma=tf.exp(  tf.Variable(np.zeros(dim_low),dtype=tf.float32,trainable=True) )\n",
    "alpha=  tf.Variable(0.1,dtype=tf.float32,trainable=False) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "meta data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_trajectory=0\n",
    "i=N_conf\n",
    "\n",
    "path_meta =dic+'ex_train.pkl'\n",
    "Z1_trajectory_meta=generate_Z1_1trajectory(path_meta,0,N_tra)\n",
    "Z_t0_meta=tf.placeholder(dtype = tf.float32, shape = [None, 1])\n",
    "Z_t1_meta=tf.placeholder(dtype = tf.float32, shape = [None, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_meta_tra =dic+'config_train_mean_every.pkl'\n",
    "X_trajectory_meta=generate_data_1trajectory(path_meta_tra,0,N_tra)\n",
    "X_t0_meta_all=X_trajectory_meta[0]@pca_1\n",
    "X_t1_meta_all=X_trajectory_meta[1]@pca_1 \n",
    "X_t_meta_all=X_trajectory_meta[2]@pca_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_pca_t0_meta= tf.placeholder(dtype = tf.float32, shape = [None,dim_low-1])\n",
    "X_pca_t1_meta= tf.placeholder(dtype = tf.float32, shape = [None,dim_low-1])\n",
    "X_pca_t_meta= tf.placeholder(dtype = tf.float32, shape = [None,dim_low-1])\n",
    "\n",
    "X_t0_tf_meta= tf.placeholder(dtype = tf.float32, shape = [None,3*N_bead])\n",
    "X_encoder_t0_NN_meta=neural_autoencoder(  X_t0_tf_meta, weights_encoder, biases_encoder)\n",
    "X_t1_tf_meta=tf.placeholder(dtype = tf.float32, shape = [None,3*N_bead])\n",
    "X_encoder_t1_NN_meta=neural_autoencoder(  X_t1_tf_meta, weights_encoder, biases_encoder)\n",
    "X_t_tf_meta=tf.placeholder(dtype = tf.float32, shape = [None,3*N_bead])\n",
    "X_encoder_t_NN_meta=neural_autoencoder(  X_t_tf_meta, weights_encoder, biases_encoder)\n",
    "Z2_low_t0_meta= X_encoder_t0_NN_meta+X_pca_t0_meta\n",
    "Z2_low_t1_meta= X_encoder_t1_NN_meta+X_pca_t1_meta\n",
    "Z2_low_t_meta=  X_encoder_t_NN_meta+X_pca_t_meta\n",
    "Z_t0_low_all_meta=tf.concat([Z_t0_meta,Z2_low_t0_meta],1)\n",
    "Z_t1_low_all_meta=tf.concat([Z_t1_meta,Z2_low_t1_meta],1)\n",
    "\n",
    "\n",
    "neural_g_meta=neural_RHS(Z_t0_low_all_meta, weights_potential, biases_potential,Gamma,beta , weights_force, biases_force,weights_A, biases_A,dim_low, alpha)\n",
    "int_g_meta=   dt/2*( ((Z_t1_low_all_meta-Z_t0_low_all_meta)/dt-neural_g_meta)*(1/(sigma[None,:])) )**2+tf.log( sigma[None,:]) #500000,dim_low\n",
    "X_H_PCA_meta=Z2_low_t_meta@((eig_vecs[:,:dim_low-1]*np.sqrt(eig_vals[:dim_low-1][None,:])).T)\n",
    "X_decoder_t_NN_meta= neural_autoencoder(  Z2_low_t_meta, weights_decoder, biases_decoder)\n",
    "X_meta_error=X_t_tf_meta-X_H_PCA_meta-X_decoder_t_NN_meta\n",
    "\n",
    "\n",
    "X_PCA_H_error=X_t_tf_meta-X_pca_t_meta@((eig_vecs[:,:dim_low-1]*np.sqrt(eig_vals[:dim_low-1][None,:])).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Mean:0' shape=(3,) dtype=float32>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int_g_train_vector=tf.reduce_mean(tf.square(  int_g_meta  ) ,0  ) \n",
    "int_g_train_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "ode_train_meta=(Z_t1_low_all_meta-Z_t0_low_all_meta)/dt-neural_g_meta\n",
    "loss_ode_meta  = tf.reduce_mean(tf.square(  ode_train_meta  )   ) \n",
    "loss_sigma_meta= tf.reduce_mean(tf.square(  sigma**2-dt*tf.reduce_mean( tf.square(  ode_train_meta  ) ,0 )  ))\n",
    "\n",
    "loss_ode=loss_ode_meta\n",
    "loss_sigma=loss_sigma_meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_ode_train_vector=tf.reduce_mean(tf.square(  ode_train_meta  ) ,0  ) \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_H_compare =tf.nn.relu( tf.log(  tf.reduce_mean(tf.square(X_meta_error))  )-tf.log(  tf.reduce_mean(tf.square(X_PCA_H_error))  )   )\n",
    "loss_H =  tf.log(  tf.reduce_mean(tf.square(X_meta_error))  )   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_meta=tf.reduce_mean(int_g_meta)  \n",
    "loss=loss_meta+100*loss_H_compare+0.001*loss_H "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "N1_test=0\n",
    "N2_test=110\n",
    "path_ex_test = dic + \"ex_test.pkl\"\n",
    "path_config_test = dic + \"config_test_mean_every.pkl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z1_trajectory_meta_test=generate_Z1_1trajectory(path_ex_test,N1_test,N2_test)\n",
    "Z_t0_meta_test=tf.placeholder(dtype = tf.float32, shape = [None,1])\n",
    "Z_t1_meta_test=tf.placeholder(dtype = tf.float32, shape = [None, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_trajectory_meta_test=generate_data_1trajectory(path_config_test,N1_test,N2_test) \n",
    "X_t0_meta_all_test=X_trajectory_meta_test[0]@pca_1\n",
    "X_t1_meta_all_test=X_trajectory_meta_test[1]@pca_1\n",
    "X_t_meta_all_test=X_trajectory_meta_test[2]@pca_1\n",
    "X_pca_t0_meta_test= tf.placeholder(dtype = tf.float32, shape = [None,dim_low-1])\n",
    "X_pca_t1_meta_test= tf.placeholder(dtype = tf.float32, shape = [None,dim_low-1])\n",
    "X_pca_t_meta_test= tf.placeholder(dtype = tf.float32, shape = [None,dim_low-1])\n",
    "\n",
    "X_t0_tf_meta_test= tf.placeholder(dtype = tf.float32, shape = [None,3*N_bead])\n",
    "X_encoder_t0_NN_meta_test=neural_autoencoder(  X_t0_tf_meta_test, weights_encoder, biases_encoder)\n",
    "X_t1_tf_meta_test=tf.placeholder(dtype = tf.float32, shape = [None,3*N_bead])\n",
    "X_encoder_t1_NN_meta_test=neural_autoencoder(  X_t1_tf_meta_test, weights_encoder, biases_encoder)\n",
    "X_t_tf_meta_test=tf.placeholder(dtype = tf.float32, shape = [None,3*N_bead])\n",
    "X_encoder_t_NN_meta_test=neural_autoencoder(  X_t_tf_meta_test, weights_encoder, biases_encoder)\n",
    "Z2_low_t0_meta_test= X_encoder_t0_NN_meta_test+X_pca_t0_meta_test\n",
    "Z2_low_t1_meta_test= X_encoder_t1_NN_meta_test+X_pca_t1_meta_test\n",
    "\n",
    "Z2_low_t_meta_test= X_encoder_t_NN_meta_test+X_pca_t_meta_test\n",
    "\n",
    "Z_t0_low_all_meta_test=tf.concat([Z_t0_meta_test,Z2_low_t0_meta_test],1)\n",
    "Z_t1_low_all_meta_test=tf.concat([Z_t1_meta_test,Z2_low_t1_meta_test],1) \n",
    "neural_g_meta_test=neural_RHS(Z_t0_low_all_meta_test, weights_potential, biases_potential,Gamma,beta , weights_force, biases_force,weights_A, biases_A,dim_low, alpha)\n",
    "int_g_meta_test=   dt/2*( ((Z_t1_low_all_meta_test-Z_t0_low_all_meta_test)/dt-neural_g_meta_test)*(1/(sigma[None,:])) )**2+tf.log( sigma[None,:]) #500000,dim_low\n",
    "\n",
    "X_H_PCA_meta_test=Z2_low_t_meta_test@((eig_vecs[:,:dim_low-1]*np.sqrt(eig_vals[:dim_low-1][None,:])).T)\n",
    "X_decoder_t_NN_meta_test= neural_autoencoder(  Z2_low_t_meta_test, weights_decoder, biases_decoder)\n",
    "X_meta_error_test=X_t_tf_meta_test-X_H_PCA_meta_test-X_decoder_t_NN_meta_test\n",
    "\n",
    "X_PCA_H_test_error=X_t_tf_meta_test-X_pca_t_meta_test@((eig_vecs[:,:dim_low-1]*np.sqrt(eig_vals[:dim_low-1][None,:])).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Mean_9:0' shape=(3,) dtype=float32>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int_g_test_vector=tf.reduce_mean(tf.square(  int_g_meta_test  ) ,0  ) \n",
    "int_g_test_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "ode_test_meta=(Z_t1_low_all_meta_test-Z_t0_low_all_meta_test)/dt-neural_g_meta_test\n",
    "loss_ode_meta_test  = tf.reduce_mean(tf.square(  ode_test_meta  )   ) \n",
    "loss_sigma_meta_test= tf.reduce_mean(tf.square(  sigma**2-dt*tf.reduce_mean( tf.square(  ode_test_meta  ) ,0 )  ))\n",
    "\n",
    "loss_ode_test= loss_ode_meta_test\n",
    "loss_sigma_test=loss_sigma_meta_test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_H_test_compare  =tf.nn.relu( tf.log(  tf.reduce_mean(tf.square(X_meta_error_test))  )-tf.log(  tf.reduce_mean(tf.square(X_PCA_H_test_error))  )   )\n",
    "\n",
    "loss_H_test  =  tf.log(  tf.reduce_mean(tf.square(X_meta_error_test))  ) \n",
    "\n",
    "loss_meta_test=tf.reduce_mean(int_g_meta_test)  \n",
    "loss_test=loss_meta_test+100*loss_H_test_compare+0.001*loss_H_test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Mean_17:0' shape=(3,) dtype=float32>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_ode_test_vector=tf.reduce_mean(tf.square(  ode_test_meta  ) ,0  ) \n",
    "loss_ode_test_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer_Adam1 = tf.train.AdamOptimizer(1e-3)\n",
    "train_op1 = optimizer_Adam1.minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor 'Log_7:0' shape=() dtype=float32>,\n",
       " <tf.Tensor 'add_47:0' shape=() dtype=float32>)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_H_test,loss_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_ode_record=[]\n",
    "loss_sigma_record=[]\n",
    "loss_ode_test_record=[]\n",
    "loss_sigma_test_record=[]\n",
    "loss_record = []\n",
    "loss_meta_record = [] \n",
    "loss_H_record=[]\n",
    "loss_H_compare_record=[]\n",
    "loss_test_record = []\n",
    "loss_meta_test_record = []  \n",
    "loss_ode_train_vector_record=[]\n",
    "loss_ode_test_vector_record=[]\n",
    "\n",
    "int_g_train_vector_record=[]\n",
    "int_g_test_vector_record=[]\n",
    "\n",
    "loss_H_test_record=[]\n",
    "loss_H_test_compare_record=[]\n",
    "loss_reg_record = []\n",
    "beta_record = []\n",
    "alpha_record=[]\n",
    "sigma_record = []\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with tf.Session(config=config) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for i in range(101):\n",
    "        choose_data=np.random.choice(X_trajectory_meta[0].shape[0],train_data, False)\n",
    "        train_dict_meta = {X_pca_t0_meta: X_t0_meta_all[choose_data,:],\n",
    "                     X_pca_t1_meta: X_t1_meta_all[choose_data,:],\n",
    "                      X_pca_t_meta:X_t_meta_all[choose_data,:],\n",
    "                     Z_t0_meta:Z1_trajectory_meta[0][choose_data,:],\n",
    "                     Z_t1_meta:Z1_trajectory_meta[1][choose_data,:],\n",
    "                      X_t0_tf_meta:X_trajectory_meta[0][choose_data,:],\n",
    "                      X_t1_tf_meta:X_trajectory_meta[1][choose_data,:],\n",
    "                      X_t_tf_meta:X_trajectory_meta[2][choose_data,:] } \n",
    "        \n",
    "        choose_data_test=np.random.choice(X_trajectory_meta_test[0].shape[0],test_data, False)\n",
    "        test_dict_meta = {X_pca_t0_meta_test: X_t0_meta_all_test[choose_data_test,:],\n",
    "                     X_pca_t1_meta_test: X_t1_meta_all_test[choose_data_test,:],\n",
    "                      X_pca_t_meta_test:X_t_meta_all_test[choose_data_test,:],\n",
    "                     Z_t0_meta_test:Z1_trajectory_meta_test[0][choose_data_test,:],\n",
    "                     Z_t1_meta_test:Z1_trajectory_meta_test[1][choose_data_test,:],\n",
    "                      X_t0_tf_meta_test:X_trajectory_meta_test[0][choose_data_test,:],\n",
    "                      X_t1_tf_meta_test:X_trajectory_meta_test[1][choose_data_test,:],\n",
    "                      X_t_tf_meta_test:X_trajectory_meta_test[2][choose_data_test,:] } \n",
    "        all_dict={**train_dict_meta, **test_dict_meta }\n",
    "        \n",
    "        sess.run(train_op1,all_dict)\n",
    "        if i % 10 == 0:\n",
    "            (loss_result,loss_meta_result, loss_test_result,loss_meta_test_result,loss_H_test_result,loss_H_result,loss_H_compare_result,loss_H_test_compare_result   ) = sess.run([loss,loss_meta ,loss_test,loss_meta_test ,loss_H_test ,loss_H ,loss_H_compare ,loss_H_test_compare  ],all_dict)\n",
    "            ( loss_ode_result,loss_sigma_result,loss_ode_test_result,loss_sigma_test_result) = sess.run([ loss_ode,loss_sigma,loss_ode_test,loss_sigma_test  ],all_dict)\n",
    "            (loss_ode_train_vector_result,loss_ode_test_vector_result,int_g_train_vector_result,int_g_test_vector_result)=sess.run([loss_ode_train_vector,loss_ode_test_vector ,int_g_train_vector,int_g_test_vector  ],all_dict)\n",
    "            int_g_train_vector_record.append(int_g_train_vector_result)\n",
    "            int_g_test_vector_record.append(int_g_test_vector_result)\n",
    " \n",
    "            loss_ode_train_vector_record.append(loss_ode_train_vector_result)\n",
    "            loss_ode_test_vector_record.append(loss_ode_test_vector_result)\n",
    "        \n",
    "            loss_H_test_record.append(loss_H_test_result)\n",
    "            loss_H_record.append(loss_H_result)\n",
    "            loss_H_test_compare_record.append(loss_H_test_compare_result)\n",
    "            loss_H_compare_record.append(loss_H_compare_result)\n",
    "            temp_sigma=sess.run(sigma,all_dict)\n",
    "            temp_alpha=sess.run(alpha,all_dict)\n",
    "            \n",
    "\n",
    "            loss_ode_record.append(loss_ode_result) \n",
    "            loss_sigma_record.append(loss_sigma_result) \n",
    "            loss_ode_test_record.append(loss_ode_test_result) \n",
    "            loss_sigma_test_record.append(loss_sigma_test_result) \n",
    "            \n",
    "            \n",
    "            loss_record.append(loss_result)\n",
    "            loss_meta_record.append(loss_meta_result) \n",
    "            loss_test_record.append(loss_test_result)\n",
    "            loss_meta_test_record.append(loss_meta_test_result) \n",
    "            \n",
    "            sigma_record.append(temp_sigma)\n",
    "            alpha_record.append(temp_alpha)\n",
    "            print ('  %d  %8.2e  %8.2e  %8.2e   %8.2e    %8.2e    %8.2e  %8.2e  %8.2e  %8.2e   %8.2e   %8.2e  %8.2e   %8.2e   %8.2e  %8.2e   %8.2e  %8.2e   %8.2e  %8.2e %8.2e  %8.2e' % (i,loss_ode_result,loss_sigma_result, loss_H_result,loss_H_compare_result,loss_meta_result ,loss_result,loss_ode_test_result,loss_sigma_test_result, loss_H_test_result,loss_H_test_compare_result,    loss_meta_test_result,loss_test_result, temp_sigma[0],temp_sigma[1],temp_sigma[2]  ,loss_ode_train_vector_result[0],loss_ode_train_vector_result[1],loss_ode_train_vector_result[2] ,int_g_train_vector_result[0],int_g_train_vector_result[1],int_g_train_vector_result[2]  ) )\n",
    "        if i% 100==0:\n",
    "            (weights_potential_np,biases_potential_np,Gamma_np,beta_np, weights_encoder_np,biases_encoder_np,weights_decoder_np,biases_decoder_np,weights_L_np,biases_L_np,weights_A_np,biases_A_np)=sess.run([weights_potential,biases_potential,Gamma,beta,  weights_encoder,biases_encoder,weights_decoder,biases_decoder ,weights_L,biases_L,weights_A,biases_A, ])\n",
    "            sample_list = {\"weights_potential\": weights_potential_np, \"biases_potential\": biases_potential_np,\"Gamma\":Gamma_np,\"beta\":beta_np,  \"weights_encoder\": weights_encoder_np, \"biases_encoder\": biases_encoder_np,\"weights_decoder\": weights_decoder_np, \"biases_decoder\": biases_decoder_np, \"weights_L\": weights_L_np, \"biases_L\": biases_L_np, \"sigma\":temp_sigma,\"weights_A\": weights_A_np, \"biases_A\": biases_A_np}\n",
    "            file_name = res_dir+'/hyper' + str(i) + '.pkl'\n",
    "            open_file = open(file_name, \"wb\")\n",
    "            pickle.dump(sample_list, open_file)\n",
    "            open_file.close()\n",
    "\n",
    "            np.savetxt(res_dir+'/loss_H-mat.txt',np.array(loss_H_record),fmt='%10.5e') \n",
    "            np.savetxt(res_dir+'/loss_H_test-mat.txt',np.array(loss_H_test_record),fmt='%10.5e') \n",
    "            np.savetxt(res_dir+'/loss_H_compare-mat.txt',np.array(loss_H_compare_record),fmt='%10.5e') \n",
    "            np.savetxt(res_dir+'/loss_H_test_compare-mat.txt',np.array(loss_H_test_compare_record),fmt='%10.5e') \n",
    "            \n",
    "            np.savetxt(res_dir+'/loss_ode-mat.txt',np.array(loss_ode_record),fmt='%10.5e') \n",
    "            np.savetxt(res_dir+'/loss_sigma-mat.txt',np.array(loss_sigma_record),fmt='%10.5e') \n",
    "            np.savetxt(res_dir+'/loss_ode_test-mat.txt',np.array(loss_ode_test_record),fmt='%10.5e') \n",
    "            np.savetxt(res_dir+'/loss_sigma_test-mat.txt',np.array(loss_sigma_test_record),fmt='%10.5e') \n",
    "            \n",
    "            np.savetxt(res_dir+'/loss-mat.txt',np.array(loss_record),fmt='%10.5e')\n",
    "            np.savetxt(res_dir+'/loss_meta-mat.txt',np.array(loss_meta_record),fmt='%10.5e') \n",
    "            np.savetxt(res_dir+'/loss_test-mat.txt',np.array(loss_test_record),fmt='%10.5e')\n",
    "            np.savetxt(res_dir+'/loss_meta_test-mat.txt',np.array(loss_meta_test_record),fmt='%10.5e') \n",
    "            np.savetxt(res_dir+'/loss_ode_train_vector.txt',np.array(loss_ode_train_vector_record),fmt='%10.5e')\n",
    "            np.savetxt(res_dir+'/loss_ode_test_vector.txt',np.array(loss_ode_test_vector_record),fmt='%10.5e')\n",
    "            np.savetxt(res_dir+'/int_g_train_vector.txt',np.array(int_g_train_vector_record),fmt='%10.5e')\n",
    "            np.savetxt(res_dir+'/int_g_test_vector.txt',np.array(int_g_test_vector_record),fmt='%10.5e')\n",
    "            np.savetxt(res_dir+'/sigma-mat.txt',np.array(sigma_record),fmt='%10.5e')\n",
    "            np.savetxt(res_dir+'/beta-mat.txt',np.array(beta_np),fmt='%10.5e')\n",
    "            np.savetxt(res_dir+'/alpha-mat.txt',np.array(alpha_record),fmt='%10.5e')\n",
    "            np.savetxt(res_dir+'/Gamma.txt',np.array(Gamma_np[0]),fmt='%10.5e')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
